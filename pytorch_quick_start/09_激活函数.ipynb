{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 函数：$S\\left ( x \\right )  = \\frac{e^x}{e^x+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 函数导数：$S'\\left ( x \\right )  = S(x)\\left ( 1-S(x) \\right ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/sigmoid.png\" alt=\"sigmoid 函数\" width=650 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100.0000,  -77.7778,  -55.5556,  -33.3333,  -11.1111,   11.1111,\n",
      "          33.3333,   55.5556,   77.7778,  100.0000])\n",
      "tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n",
      "tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "z = torch.linspace(-100, 100, 10)\n",
    "print(z)\n",
    "\n",
    "print(torch.sigmoid(z))\n",
    "print(F.sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = tanh(x) = \\frac{e^x-\\frac{1}{e^x}  }{e^x+\\frac{1}{e^x}} =2sigmoid(2x)-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f'(x) = tanh'(x) = 1-tanh^2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/tanh.png\" alt=\"tanh 函数\" width=650 height=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n",
      "tensor([-0.7616, -0.6514, -0.5047, -0.3215, -0.1107,  0.1107,  0.3215,  0.5047,\n",
      "         0.6514,  0.7616])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "z = torch.linspace(-1, 1, 10)\n",
    "print(z)\n",
    "\n",
    "print(torch.tanh(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/ReLU.png\" alt=\"relu 函数\" width=1000 height=350/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n",
      "        1.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "z = torch.linspace(-1, 1, 10)\n",
    "print(z)\n",
    "\n",
    "print(torch.relu(z))\n",
    "print(F.relu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 7, 7])\n",
      "torch.Size([1, 16, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.rand(1, 16, 7, 7)\n",
    "# inplace=True 表示会直接更新 x（这样会减小内存消耗）\n",
    "layer = nn.ReLU(inplace=True)\n",
    "out = layer(x)\n",
    "print(out.shape)\n",
    "\n",
    "out = F.relu(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft version of max</br>**softmax函数用于多分类过程中**，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/softmax.png\" alt=\"relu 函数\" width=1000 height=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax 激活函数可以将上一层的原始数据进行归一化，转化为一个  [0, 1] 之间的数值，这些数值可以被当做概率分布，用来作为多分类的目标预测值。</br>Softmax函数一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。**之所以要选用e作为底数的指数函数来转换概率，是因为上一层的输出有正有负，采用指数函数可以将其第一步都变成大于0的值，然后再算概率分布**。</br>同时，softmax 还能加大数值最大和数值次大之间的差距"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/softmax_loss1.jpeg\" alt=\"relu 函数\" width=600 height=150/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/softmax_loss2.jpeg\" alt=\"relu 函数\" width=600 height=150/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/softmax_loss1_exp_x1.jpeg\" alt=\"relu 函数\" width=1300 height=150/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/softmax_loss1_exp_x2.jpeg\" alt=\"relu 函数\" width=880 height=150/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6229, 0.1372, 0.4505], requires_grad=True)\n",
      "tensor([0.4070, 0.2504, 0.3426], grad_fn=<SoftmaxBackward0>)\n",
      "None\n",
      "(tensor([-0.1019,  0.1877, -0.0858]),)\n",
      "(tensor([-0.1394, -0.0858,  0.2252]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17107\\AppData\\Local\\Temp\\ipykernel_9464\\827456606.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(p[1].grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F \n",
    "\n",
    "z = torch.rand(3, requires_grad=True)\n",
    "print(z)\n",
    "\n",
    "# 返回一个 probability 概率\n",
    "p = F.softmax(z, dim=0)\n",
    "print(p)\n",
    "\n",
    "# RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "# 只有对标量输出它才会计算梯度，而求一个矩阵对另一矩阵的导数束手无策，解决方法是给backward加上参数grad_tensors，相当于z和一个大小相同的矩阵相乘后再求导\n",
    "# grad_tensors的作用其实可以简单地理解成在求梯度时的权重，因为可能不同值的梯度对结果影响程度不同\n",
    "p.backward(torch.ones_like(p)) # grad_tensors 需要与输入 tensor p 大小一致\n",
    "\n",
    "p = F.softmax(z, dim=0)\n",
    "# 计算梯度\n",
    "# 第一个参数是 loss 函数，第二个参数是个要被函数求偏导的所有自变量组成的列表\n",
    "print(torch.autograd.grad(p[1], [z], retain_graph=True))\n",
    "print(torch.autograd.grad(p[2], [z]))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
