{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6230e+05, 1.8932e-42, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个 5×3 的矩阵\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6926, 0.1102, 0.1727],\n",
      "        [0.6507, 0.8499, 0.5912],\n",
      "        [0.8212, 0.4162, 0.2538],\n",
      "        [0.1119, 0.6597, 0.0214],\n",
      "        [0.9122, 0.9295, 0.0047]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个随机初始化的 5*3 矩阵\n",
    "rand_x = torch.rand(5, 3)\n",
    "print(rand_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个数值皆是 0，类型为 long 的矩阵\n",
    "zero_x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(zero_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 直接用列表创建\n",
    "tensor1 = torch.tensor([5.5, 3])\n",
    "print(tensor1)\n",
    "\n",
    "# 二维列表\n",
    "tensor2 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor1 = torch.tensor([5.5, 3])\n",
    "print(tensor1)\n",
    "# 显示定义新的尺寸是 5*3，数值类型是 torch.double\n",
    "  # new_* 方法需要输入 tensor 大小\n",
    "tensor3 = tensor1.new_ones(5, 3, dtype=torch.double)\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[ 1.2256,  0.8793, -1.2986],\n",
      "        [-1.0772, -0.4383, -0.7421],\n",
      "        [-0.7059,  1.0707,  0.8640]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor2 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(tensor2)\n",
    "# 修改数值类型\n",
    "tensor4 = torch.randn_like(tensor2, dtype=torch.float)\n",
    "print(tensor4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor5 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(tensor5.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+运算符\n",
      "tensor([[1.6342, 2.9043, 3.2132],\n",
      "        [4.7699, 5.4462, 6.2296],\n",
      "        [7.9417, 8.6639, 9.8168]], dtype=torch.float64)\n",
      "torch.add(tensor1, tensor2, [out=tensor3])\n",
      "tensor([[1.6342, 2.9043, 3.2132],\n",
      "        [4.7699, 5.4462, 6.2296],\n",
      "        [7.9417, 8.6639, 9.8168]])\n",
      "tensor([[1.6342, 2.9043, 3.2132],\n",
      "        [4.7699, 5.4462, 6.2296],\n",
      "        [7.9417, 8.6639, 9.8168]])\n",
      "tensor1.add_(tensor2)\n",
      "tensor([[1.6342, 2.9043, 3.2132],\n",
      "        [4.7699, 5.4462, 6.2296],\n",
      "        [7.9417, 8.6639, 9.8168]], dtype=torch.float64)\n",
      "tensor([[1.6342, 2.9043, 3.2132],\n",
      "        [4.7699, 5.4462, 6.2296],\n",
      "        [7.9417, 8.6639, 9.8168]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "tensor6 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)\n",
    "tensor7 = torch.rand(3, 3)\n",
    "\n",
    "print('+运算符')\n",
    "print(tensor6 + tensor7)\n",
    "print('torch.add(tensor1, tensor2, [out=tensor3])')\n",
    "# 声明一个 3×3 的未初始化的矩阵，用于保存加法操作结果\n",
    "tensor8 = torch.empty(3, 3)\n",
    "print(torch.add(tensor6, tensor7, out=tensor8))\n",
    "print(tensor8)\n",
    "# 直接修改变量\n",
    "print('tensor1.add_(tensor2)')\n",
    "print(tensor6.add_(tensor7))\n",
    "print(tensor6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1671, 0.2738, 0.9056, 0.3052],\n",
      "        [0.8883, 0.0234, 0.9751, 0.5849],\n",
      "        [0.3800, 0.9318, 0.1067, 0.2627],\n",
      "        [0.5814, 0.3606, 0.2599, 0.1000],\n",
      "        [0.9681, 0.5262, 0.9840, 0.3978]])\n",
      "tensor([0.1671, 0.8883, 0.3800, 0.5814, 0.9681])\n",
      "tensor([[0.8883, 0.0234, 0.9751, 0.5849],\n",
      "        [0.3800, 0.9318, 0.1067, 0.2627],\n",
      "        [0.5814, 0.3606, 0.2599, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor9 = torch.rand(5, 4)\n",
    "print(tensor9)\n",
    "# 访问 tensor9 第一列数据\n",
    "print(tensor9[:, 0])\n",
    "# 访问 tensor9 第2-4行数据\n",
    "print(tensor9[1:4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "# -1 表示除给定维度外的其余维度的乘积\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7281])\n",
      "0.728061854839325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7281], device='cuda:0')\n",
      "tensor([1.7281], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 当 CUDA 可用的时候，可用运行下方这段代码，采用 torch.device() 方法来改变 tensors 是否在 GPU 上进行计算操作\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # 定义一个 CUDA 设备对象\n",
    "    y = torch.ones_like(x, device=device)  # 显示创建在 GPU 上的一个 tensor\n",
    "    x = x.to(device)                       # 也可以采用 .to(\"cuda\") \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # .to() 方法也可以改变数值类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# 开始创建一个 tensor， 并设置 requires_grad=True 来追踪该变量相关的计算操作：\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 执行任意计算操作，这里进行简单的加法运算：\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000013EC8F859A0>\n"
     ]
    }
   ],
   "source": [
    "# y 是一个操作的结果，所以它带有属性 grad_fn：\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z= tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "out= tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 继续对变量 y 进行操作：\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print('z=', z)\n",
    "print('out=', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000013EC8E97760>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "# 输出梯度 d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 152.5833,  684.7393, -792.4349], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "# tensor1.data：返回一个和 tensor1 有着相同数据的 tensor，且返回的新 tensor 与 tensor1 共享同一个内存空间\n",
    "# tensor.data.norm()：它对张量y每个元素进行平方，然后对它们求和，最后取平方根。 这些操作计算就是所谓的L2或欧几里德范数 \n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像是单通道，conv1 kenrnel size=5*5，输出通道 6\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # conv2 kernel size=5*5, 输出通道 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # max-pooling 采用一个 (2,2) 的滑动窗口\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 核(kernel)大小是方形的话，可仅定义一个数字，如 (2,2) 用 2 即可\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # 除了 batch 维度外的所有维度\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "# 打印网络结构\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数数量:  10\n",
      "第一个参数大小:  torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print('参数数量: ', len(params))\n",
    "# conv1.weight\n",
    "print('第一个参数大小: ', params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0475,  0.1172,  0.0313, -0.1010,  0.0285,  0.1629,  0.0408, -0.0386,\n",
      "          0.0118,  0.1917]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 随机定义一个变量输入网络\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空所有参数的梯度缓存，然后计算随机梯度进行反向传播\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2155, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "# 定义伪标签\n",
    "target = torch.randn(10)\n",
    "# 调整大小，使得和 output 一样的 size\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x000001EEFE58FFA0>\n",
      "<AddmmBackward0 object at 0x000001EEFE441C70>\n",
      "<AccumulateGrad object at 0x000001EEFE58FFA0>\n"
     ]
    }
   ],
   "source": [
    "# MSELoss\n",
    "print(loss.grad_fn)\n",
    "# Linear layer\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "# Relu\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "None\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0171,  0.0008, -0.0021,  0.0232, -0.0166, -0.0029])\n"
     ]
    }
   ],
   "source": [
    "# 清空所有参数的梯度缓存\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
